{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FMD.ipynb","provenance":[{"file_id":"https://github.com/ngoctran24/machineLearning/blob/master/FMD.ipynb","timestamp":1598554477784}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"L8EFO3aOmX53","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598885993750,"user_tz":-420,"elapsed":1169,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"7115a9d1-11b1-4b4d-9cac-fe39c559f66f"},"source":["from google.colab import drive\n","import os\n","\n","drive.mount(\"/content/gdrive\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_12bA0J1eyDG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598885994122,"user_tz":-420,"elapsed":1516,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"f51c719a-25a7-459b-88af-6f5b6c523f47"},"source":["os.chdir('/content/gdrive/My Drive/High Dimension Analysis/Practice/Final/FaceMaskDataset')\n","print(os.getcwd())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/High Dimension Analysis/Practice/Final/FaceMaskDataset\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NM5gTu6BfG1X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885994123,"user_tz":-420,"elapsed":1497,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# USAGE\n","# python train_mask_detector.py --dataset dataset\n","\n","# import the necessary packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import cv2"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaiqRXKp9HW9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885994124,"user_tz":-420,"elapsed":1487,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["def ShowImage(ImageList, nRows = 1, nCols = 2, WidthSpace = 0.00, HeightSpace = 0.00):    \n","    from matplotlib import pyplot as plt     \n","    import matplotlib.gridspec as gridspec        \n","    \n","    gs = gridspec.GridSpec(nRows, nCols)         \n","    gs.update(wspace=WidthSpace, hspace=HeightSpace) # set the spacing between axes.    \n","    plt.figure(figsize=(20,20))    \n","    for i in range(len(ImageList)):        \n","        ax1 = plt.subplot(gs[i])        \n","        ax1.set_xticklabels([])        \n","        ax1.set_yticklabels([])        \n","        ax1.set_aspect('equal') \n","        \n","        plt.subplot(nRows, nCols,i+1) \n","        image = ImageList[i].copy()        \n","        if (len(image.shape) < 3):            \n","            plt.imshow(image, plt.cm.gray)        \n","        else:            \n","            plt.imshow(image)        \n","            plt.title(\"Image \" + str(i))        \n","            plt.axis('off') \n","    plt.show()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jlnuth5-VprT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885994125,"user_tz":-420,"elapsed":1477,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["def get_subfiles(dir):\n","  \"Get a list of immediate subfiles\"\n","  return next(os.walk(dir))[2]\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhcOm3Xf22Fw","colab_type":"text"},"source":["## **Save image**"]},{"cell_type":"code","metadata":{"id":"i6BLgvW129Hc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885988625,"user_tz":-420,"elapsed":743,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# def save_data(path, no_of_images, path_p): #function to make dataset\n","#     for i,j in enumerate(no_of_images):\n","#         if ((i%20 == 0) and (i != 0)):\n","#             print()\n","#         print(i, end = ' ')\n","#         tree = ElementTree.parse(path+j[:-4]+\".xml\")\n","#         root = tree.getroot()\n","\n","#         if (root.find('object') == None):\n","#             continue\n","\n","#         label_dt = []\n","#         for l in root.findall('.//name'):\n","#             if (l.text not in options):\n","#                 if ('_' in l.text):\n","#                     label = options['face_mask']\n","#                 else:\n","#                     label = options['face']\n","#             else:\n","#                 label=options[l.text]\n","#             label_dt.append(label)\n","\n","#         t = 0\n","#         for box in root.findall('.//bndbox'):\n","#             a = int(box.find('xmin').text)\n","#             b = int(box.find('ymin').text)\n","#             c = int(box.find('xmax').text)+50\n","#             d = int(box.find('ymax').text)\n","\n","#             image = cv2.imread(path+j)\n","#             # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","#             image = image[b:d, a:c]\n","#             area = image.shape[0]*image.shape[1]\n","#             image = cv2.resize(image, (224, 224))\n","#             if (area < 3000 and label_dt[t] == 0):\n","#                 del label_dt[t]\n","#                 # j -= 1\n","#                 continue\n","#             if (label_dt[t] == 0):\n","#                 cv2.imwrite(path_p+'/face/'+str(i)+'_'+str(t)+'.png', image)\n","#             else:\n","#                 cv2.imwrite(path_p+'/face_mask/'+str(i)+'_'+str(t)+'.png', image)\n","#             t += 1"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"-H0_4DeU3fUe","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885988627,"user_tz":-420,"elapsed":736,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# path_train = 'image_processing/train/'\n","# make_dataset(path_train)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"77gf3Tfc3kML","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885988629,"user_tz":-420,"elapsed":730,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# path_val = 'image_processing/val/'\n","# make_dataset(path_val)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKXxUcxve3gC","colab_type":"text"},"source":["## **Train model**"]},{"cell_type":"code","metadata":{"id":"xpQqoYVMmdsc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885988630,"user_tz":-420,"elapsed":717,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# USAGE\n","# python train_mask_detector.py --dataset dataset\n","\n","# import the necessary packages\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from imutils import paths\n","import matplotlib.pyplot as plt"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTFug91dnzKu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598885988632,"user_tz":-420,"elapsed":711,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"1da50776-fd6b-4818-e90d-4f7d4a303872"},"source":["# grab the list of images in our dataset directory, then initialize\n","# the list of data (i.e., images) and class images\n","print(\"[INFO] loading images...\")\n","# imagePaths = '/content/gdrive/My Drive/shecode/reference2-paper/dataset/'\n","\n","# os.path.exists(imagePaths)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[INFO] loading images...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qY6hEeedMgWC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598885988632,"user_tz":-420,"elapsed":702,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["def get_image_label(path):\n","\t\tdata = []\n","\t\tlabels = []\n","\t\tfor imagePath in os.listdir(path):\n","\t\t\t# extract the class label from the filename\n","\t\t\t\tlabel = imagePath\n","\t\t\t\ti = 0\n","\t\t\t\tprint('\\n',imagePath)\n","\t\t\t\tfor name in get_subfiles(path + imagePath):\n","\t\t\t\t\t\t# load the input image (224x224) and preprocess it\n","\t\t\t\t\t\tif (i == 6000):\n","\t\t\t\t\t\t\t\tbreak\n","\t\t\t\t\t\tif ((i%20 == 0) and (i != 0)):\n","\t\t\t\t\t\t\t\tprint()\n","\t\t\t\t\t\tprint(i, end = ' ')\n","\t\t\t\t\t\timg = cv2.imread(path + imagePath + '/' + name)\n","\t\t\t\t\t\t# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\t\t\t\t\t\t# img = cv2.resize(img, (224, 224))\n","\t\t\t\t\t\timg_ = img_to_array(img)\n","\t\t\t\t\t\timage = preprocess_input(img_)\n","\n","\t\t\t\t\t\t# update the data and labels lists, respectively\n","\t\t\t\t\t\tdata.append(image)\n","\t\t\t\t\t\tlabels.append(label)\n","\t\t\t\t\t\ti += 1\n","\t\treturn data, labels"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"NI0Z1JSLYYq2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"error","timestamp":1598885988633,"user_tz":-420,"elapsed":692,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"a087217a-97d5-475e-96b2-4275869bf424"},"source":["trainX, trainY = get_image_label('FaceMaskDataset/image_processing/train/')\n","\n","# convert the data and labels to NumPy arrays\n","trainX = np.array(trainX, dtype=\"float32\")\n","trainY = np.array(trainY)\n","\n","lb = LabelBinarizer()\n","trainY = lb.fit_transform(trainY)\n","trainY = to_categorical(trainY)"],"execution_count":12,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ba78f6cc2ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FaceMaskDataset/image_processing/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# convert the data and labels to NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-4a00f93aefa9>\u001b[0m in \u001b[0;36mget_image_label\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mimagePath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                         \u001b[0;31m# extract the class label from the filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimagePath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FaceMaskDataset/image_processing/train/'"]}]},{"cell_type":"code","metadata":{"id":"rd6ty1pnbgWD","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988636,"user_tz":-420,"elapsed":675,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["valX, valY = get_image_label('FaceMaskDataset/image_processing/val/')\n","\n","# convert the data and labels to NumPy arrays\n","valX = np.array(valX, dtype=\"float32\")\n","valY = np.array(valY)\n","\n","lb = LabelBinarizer()\n","valY = lb.fit_transform(valY)\n","valY = to_categorical(valY)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7j5GOFzopFRe","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988650,"user_tz":-420,"elapsed":656,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# initialize the initial learning rate, number of epochs to train for, and batch size\n","INIT_LR = 1e-4\n","EPOCHS = 20\n","BS = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vl5GbeZZR4L","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988651,"user_tz":-420,"elapsed":637,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# TRAIN\n","# construct the training image generator for data augmentation\n","# load the MobileNetV2 network, ensuring the head FC layer sets are left off\n","baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n","                        input_tensor=Input(shape=(224, 224, 3)))\n","\n","# construct the head of the model that will be placed on top of the\n","# the base model\n","headModel = baseModel.output\n","headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n","headModel = Flatten(name=\"flatten\")(headModel)\n","headModel = Dense(128, activation=\"relu\")(headModel)\n","headModel = Dropout(0.5)(headModel)\n","headModel = Dense(2, activation=\"softmax\")(headModel)\n","\n","# place the head FC model on top of the base model (this will become\n","# the actual model we will train)\n","model = Model(inputs=baseModel.input, outputs=headModel)\n","\n","# loop over all layers in the base model and freeze them so they will\n","# *not* be updated during the first training process\n","for layer in baseModel.layers:\n","\t\tlayer.trainable = False\n","\n","# compile our model\n","print(\"[INFO] compiling model...\")\n","\n","opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n","              metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bi7sbMzwpNLN","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988654,"user_tz":-420,"elapsed":621,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# train the head of the network\n","print(\"[INFO] training head...\")\n","H = model.fit(trainX, trainY, \n","              batch_size=BS,\n","              steps_per_epoch=len(trainX) // BS,\n","              validation_data=(valX, valY),\n","              validation_steps=len(valX) // BS,\n","              epochs=EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiKTXrfntUT3","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988656,"user_tz":-420,"elapsed":605,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# make predictions on the testing set\n","print(\"[INFO] evaluating network...\")\n","predIdxs = model.predict(valX, batch_size=BS)\n","\n","# for each image in the testing set we need to find the index of the\n","# label with corresponding largest predicted probability\n","predIdxs = np.argmax(predIdxs, axis=1)\n","\n","# show a nicely formatted classification report\n","print(classification_report(valY.argmax(axis=1), \n","                            predIdxs,\n","                            target_names=lb.classes_))\n","\n","# serialize the model to disk\n","print(\"[INFO] saving mask detector model...\")\n","model.save(\"model.h5\")\n","\n","# serialize model to JSON\n","# model_json = model.to_json()\n","# with open(\"./save_model/model.json\", \"w\") as json_file:\n","#    json_file.write(model_json)\n","# serialize weights to HDF5\n","# model.save_weights(\"./save_model/model.h5\")\n","print(\"Saved model to disk\")\n","model.summary()\n","\n","\n","# plot the training loss and accuracy\n","N = EPOCHS\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","# plt.savefig(args[\"plot\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OlWQaYWqag_V","colab_type":"text"},"source":["## **Load model**"]},{"cell_type":"code","metadata":{"id":"EFZNTIHB3Ufo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598886077331,"user_tz":-420,"elapsed":15475,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"05bdb07e-99ea-4f90-d162-f225f0d22ddf"},"source":["# USAGE\n","# python detect_mask_image.py --image examples/example_01.png\n","\n","# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import cv2\n","import os\n","\n","print(\"[INFO] loading face mask detector model...\")\n","model = load_model('model.h5')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[INFO] loading face mask detector model...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GvT8UYCsuGb8","colab_type":"text"},"source":["## **DETECT MASK IMAGE**"]},{"cell_type":"code","metadata":{"id":"OhGV-2myK3oc","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988659,"user_tz":-420,"elapsed":575,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["testImg = []\n","pathPredict = \"/content/gdrive/My Drive/High Dimension Analysis/Practice/Final/FaceMaskDataset/test/\"\n","for name in get_subfiles(pathPredict):\n","\t# load the input image (224x224) and preprocess it\n","\timg = cv2.imread(pathPredict + name)\n","\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\ttestImg.append(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgpNT-9EB0Gu","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988660,"user_tz":-420,"elapsed":562,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["ShowImage(testImg, 1, len(testImg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0BoTS8RDH7f","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988662,"user_tz":-420,"elapsed":549,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["!pip install mtcnn #installing library for predicting faces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PyM_6fjTDBye","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988663,"user_tz":-420,"elapsed":532,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["from mtcnn import MTCNN\n","detect = MTCNN()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nbqHuivPL8K","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988665,"user_tz":-420,"elapsed":521,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["def testing(image_orig):\n","    options = {0:\"face_mask\", 1:\"face\"} # mapping for predictions and analysis purpose\n","    image = image_orig.copy()\n","    detections = detect.detect_faces(image)\n","    for i in range(0, len(detections)):\n","      # extract the confidence (i.e., probability) associated with\n","      # the detection\n","      # confidence = detections[0, 0, i, 2]\n","      # filter out weak detections by ensuring the confidence is\n","      # greater than the minimum confidence\n","      if detections[i]['confidence'] > 0.9: #args[\"confidence\"]:\n","        # compute the (x, y)-coordinates of the bounding box for\n","        # the object\n","        # box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","        (x, y, width, height) = detections[i]['box']\n","        (x, y) = (max(0, x), max(0, y))\n","        # ensure the bounding boxes fall within the dimensions of the frame\n","\n","        # extract the face ROI, convert it from BGR to RGB channel\n","        # ordering, resize it to 224x224, and preprocess it\n","        face = image[y:(y+height), x:(x+width)]\n","\n","        # face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","        face = cv2.resize(face, (224, 224))\n","        face = img_to_array(face)\n","        face = preprocess_input(face)\n","        face = np.expand_dims(face, axis=0)\n","\n","        # pass the face through the model to determine if the face\n","        # has a mask or not\n","        (mask, withoutMask) = model.predict(face)[0]\n","\n","        # determine the class label and color we'll use to draw\n","        # the bounding box and text\n","        label = 1 if mask > withoutMask else 0\n","        color = (0, 255, 0) if label == 1 else (255, 0, 255)\n","\n","        # include the probability in the label\n","        label = \"{}: {:.2f}%\".format(options[label], max(mask, withoutMask) * 100)\n","\n","        # display the label and bounding box rectangle on the output\n","        # frame\n","        cv2.putText(image, label, (x, y-10),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","        cv2.rectangle(image, (x, y), (x+width, y+height), color, 2)\n","    \n","    ShowImage([image_orig, image], 1, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nkm2muY4EM92","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988666,"user_tz":-420,"elapsed":507,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["testing(testImg[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p19ivBksQ_B9","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988667,"user_tz":-420,"elapsed":496,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["testing(testImg[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmoGm6PJbfPm","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988669,"user_tz":-420,"elapsed":484,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["testing(testImg[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o05oG5vCbl6B","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988670,"user_tz":-420,"elapsed":473,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["testing(testImg[3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P86elDAWt84x","colab_type":"text"},"source":["## **DETECT MASK VIDEO**"]},{"cell_type":"code","metadata":{"id":"tu3HHgCdCUI8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598886154611,"user_tz":-420,"elapsed":2170,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"1cf11d3c-5d85-4115-c4dd-1086b553a913"},"source":["# USAGE\n","# python detect_mask_image.py --image examples/example_01.png\n","\n","# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import cv2\n","import os\n","\n","print(\"[INFO] loading face mask detector model...\")\n","model = load_model('model.h5')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[INFO] loading face mask detector model...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_i8sY_sOCsQD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1598886172303,"user_tz":-420,"elapsed":3654,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}},"outputId":"6c4f5c32-c6d1-47ac-a166-c54d24c261c7"},"source":["!pip install mtcnn #installing library for predicting faces"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Collecting mtcnn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (4.1.2.30)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (2.4.3)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.18.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n","Installing collected packages: mtcnn\n","Successfully installed mtcnn-0.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Te-Fj5hsCN1c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598886172304,"user_tz":-420,"elapsed":2923,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["from mtcnn import MTCNN\n","detect = MTCNN()"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"S6L_aJNot0uc","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598886079082,"user_tz":-420,"elapsed":1740,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["# USAGE\n","# python detect_mask_video.py\n","\n","# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from imutils.video import VideoStream\n","import numpy as np\n","import imutils\n","import time\n","import cv2\n","import os\n","\n","def detect_and_predict_mask(frame):\n","    detections = detect.detect_faces(frame)\n","    # initialize our list of faces, their corresponding locations,\n","    # and the list of predictions from our face mask network\n","    faces = []\n","    locs = []\n","    preds = []\n","\n","    # loop over the detections\n","    for i in range(0, len(detections)):\n","        # filter out weak detections by ensuring the confidence is\n","        # greater than the minimum confidence\n","        if detections[i]['confidence'] > 0.9:\n","        # compute the (x, y)-coordinates of the bounding box for the object\n","            (x, y, width, height) = detections[i]['box']\n","            (x, y) = (max(0, x), max(0, y))\n","\n","            # extract the face ROI, convert it from BGR to RGB channel\n","            # ordering, resize it to 224x224, and preprocess it\n","            face = frame[y:(y+height), x:(x+width)]\n","            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","            face = cv2.resize(face, (224, 224))\n","            face = img_to_array(face)\n","            face = preprocess_input(face)\n","\n","            # add the face and bounding boxes to their respective\n","            # lists\n","            faces.append(face)\n","            locs.append((x, y, x+width, y+height))\n","\n","    # only make a predictions if at least one face was detected\n","    if len(faces) > 0:\n","        # for faster inference we'll make batch predictions on *all*\n","        # faces at the same time rather than one-by-one predictions\n","        # in the above `for` loop\n","        faces = np.array(faces, dtype=\"float32\")\n","        preds = model.predict(faces, batch_size=32)\n","\n","    # return a 2-tuple of the face locations and their corresponding\n","    # locations\n","    return (locs, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OieNK5gAcc4","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988676,"user_tz":-420,"elapsed":421,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":["print(\"[INFO] starting video stream...\")\n","vs = VideoStream(src=0).start()\n","time.sleep(2.0)\n","\n","while True:\n","    # grab the frame from the threaded video stream and resize it\n","    # to have a maximum width of 400 pixels\n","    frame = vs.read()\n","    frame = imutils.resize(frame, width=900)\n","    # detect faces in the frame and determine if they are wearing a\n","    # face mask or not\n","    (locs, preds) = detect_and_predict_mask(frame)\n","    options = {0:\"face_mask\", 1:\"face\"}\n","    \n","    for (box, pred) in zip(locs, preds):\n","        # unpack the bounding box and predictions\n","        (startX, startY, endX, endY) = box\n","        (mask, withoutMask) = pred\n","        # determine the class label and color we'll use to draw\n","        # the bounding box and text\n","        label = 1 if mask > withoutMask else 0\n","        color = (0, 255, 0) if label == 0 else (255, 0, 255)\n","        # include the probability in the label\n","        label = \"{}: {:.2f}%\".format(options[label], max(mask, withoutMask) * 100)\n","        # display the label and bounding box rectangle on the output\n","        # frame\n","        cv2.putText(frame, label, (startX, startY - 10),\n","            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","    cv2.imshow(\"Frame\", frame)\n","    key = cv2.waitKey(1) & 0xFF\n","    # if the `q` key was pressed, break from the loop\n","    if key == ord(\"q\"):\n","        break\n","\n","cv2.destroyAllWindows()\n","vs.stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEWu990yt0n4","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598885988678,"user_tz":-420,"elapsed":407,"user":{"displayName":"Ngọc Trần","photoUrl":"","userId":"14321653692242760158"}}},"source":[""],"execution_count":null,"outputs":[]}]}